---
title: "GBIF Data Setup"
author: "Brian Yandell"
date: "2024-10-24"
output: html_document
params:
  species: "Grus americana"
  gbif_dir: "gbif_grus"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<https://www.allaboutbirds.org/guide/Whooping_Crane/overview>
<https://explorer.audubon.org/>

```{r}
for(i in dir("R")) {
  source(file.path("R", i))
}
```

This has some general things about GBIF account
and some specific things about getting single species data.

```{r}
gbif_credentials(FALSE)
```

```{r}
# Create data directory in the home folder
data_dir <- file.path("data", "species")
dir.create(data_dir, showWarnings = FALSE, recursive = TRUE)

# Define the directory name for GBIF data
gbif_dir <- file.path(data_dir, params$gbif_dir)
dir.create(gbif_dir, showWarnings = FALSE)
gbif_dir
```

The GBIF species lookup is fuzzy, does not appear to be exact, and the 
species key (`nubKey`) seems to vary.
Here, rather than picking the first, I look for `species` match
and most frequent `nubKey`.

```{r}
species_info <- rgbif::name_lookup(params$species, rank='SPECIES')
```

```{r}
dplyr::select(species_info$data[1,], scientificName, species, nubKey)
```

```{r}
nrow(species_info$data)
```

```{r}
# Query species
knitr::kable(dplyr::count(species_info$data, scientificName, species, nubKey))
```

```{r}
dplyr::filter(species_info$data,
                    .data$species == params$species) |>
  dplyr::count(scientificName, species, nubKey)
```

```{r}
# Get the first result
(species_data <- 
  dplyr::filter(
    dplyr::count(
      dplyr::filter(species_info$data,
                    .data$species == params$species,
                    !is.na(.data$nubKey)),
        species, nubKey),
    n == max(n)))
```

```{r}
knitr::knit_exit()
```

Use species key (nubKey) to pull data.
**This part not proofed yet.**

```{r}
species_key <- species_data$nubKey[1]

# Download data from GBIF
gbif_pattern <- file.path(gbif_dir, '*.csv')

if (length(list.files(gbif_dir, pattern = "*.csv")) == 0) {
    # Only submit one request
    if (is.null(Sys.getenv("GBIF_DOWNLOAD_KEY"))) {
        # Submit query to GBIF
        gbif_query <- rgbif::occ_download(
            rgbif::pred("speciesKey", 2474953),
            rgbif::pred("hasCoordinate", TRUE),
            rgbif::pred("year", 2023)
        )
        Sys.setenv(GBIF_DOWNLOAD_KEY = gbif_query$key)
    }

    # Wait for the download to build
    download_key <- Sys.getenv("GBIF_DOWNLOAD_KEY")
    wait <- rgbif::occ_download_meta(download_key)$status
    while (wait != 'SUCCEEDED') {
        wait <- rgbif::occ_download_meta(download_key)$status
        Sys.sleep(5)
    }

    # Download GBIF data
    download_info <- rgbif::occ_download_get(
        Sys.getenv("GBIF_DOWNLOAD_KEY"), 
        path = data_dir
    )

    # Unzip GBIF data
    utils::unzip(download_info$path, exdir = gbif_dir)
}

# Find the extracted .csv file path (take the first result)
gbif_path <- list.files(gbif_dir, pattern = "*.csv", full.names = TRUE)[1]
gbif_path
```

We can use ZIP file directly.

```{r}
gbif_path <- list.files(file.path(gbif_dir, "sandhill"),
                        pattern = "*.zip", full.names = TRUE)[1]
```

```{r}
# Read the GBIF data
gbif_df <- readr::read_delim(
    gbif_path, 
    delim = "\t",
    col_names = TRUE,
    col_types = readr::cols(.default = "c"),
    skip = 0) |>
  dplyr::select(gbifID, month, year, countryCode, stateProvince, decimalLatitude, decimalLongitude)
```

Probably do not need individual observations.

```{r}
gdf_monthly <- dplyr::count(gbif_df, month, year,
  countryCode, stateProvince, decimalLatitude, decimalLongitude)
```

**Not tested past here.**

```{r}
# Set up the ecoregion boundary URL
ecoregions_url <- "https://storage.googleapis.com/teow2016/Ecoregions2017.zip"

# Set up a path to save the data on your machine
ecoregions_dir <- file.path(data_dir, 'wwf_ecoregions')

# Make the ecoregions directory
dir.create(ecoregions_dir, showWarnings = FALSE, recursive = TRUE)

# Join ecoregions shapefile path
ecoregions_path <- file.path(ecoregions_dir, 'wwf_ecoregions.shp')

# Only download once. Do this by hand: go to URL, download file, unzip.
if (!file.exists(ecoregions_path)) {
    ecoregions_gdf <- sf::st_read(ecoregions_url)
    sf::st_write(ecoregions_gdf, ecoregions_path)
}
```
