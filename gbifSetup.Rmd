---
title: "GBIF Data Setup"
author: "Brian Yandell"
date: "2024-10-24"
output: html_document
params:
  species: "Whooping Crane"
  gbif_dir: "gbif_grus"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<https://www.allaboutbirds.org/guide/Whooping_Crane/overview>
<https://explorer.audubon.org/>

```{r}
for(i in dir("R")) {
  source(file.path("R", i))
}
```

This has some general things about GBIF account
and some specific things about getting single species data.

From other student working on Sandhill Cranes:

- <https://www.gbif.org/occurrence/download/0014700-241007104925546>
- <https://doi.org/10.15468/dl.4g5csj>

```{r}
gbif_credentials(FALSE)
```

```{r}
# Create data directory in the home folder
data_dir <- file.path("data", "species")
dir.create(data_dir, showWarnings = FALSE, recursive = TRUE)

# Define the directory name for GBIF data
gbif_dir <- file.path(data_dir, params$gbif_dir)
dir.create(gbif_dir, showWarnings = FALSE)
gbif_dir
```

The GBIF species lookup is fuzzy, does not appear to be exact, and the 
species key (`nubKey`) seems to vary.
Here, rather than picking the first, I look for `species` match
and most frequent `nubKey`.

```{r}
species_info <- rgbif::name_lookup(params$species, rank='SPECIES')
```

```{r}
dplyr::select(species_info$data[1,], species, key, nubKey)
```

```{r}
nrow(species_info$data)
```

```{r}
# Query species
dplyr::arrange(species_info$data, key) |>
  dplyr::group_by(species, nubKey) |>
  dplyr::summarize(count = dplyr::n(),
                   key = paste(key, collapse = ","),
                   .groups = "drop") |>
  dplyr::ungroup() |>
  dplyr::arrange(species, nubKey) |>
  knitr::kable()
```

Find out more about the `key` names.

```{r}
dplyr::arrange(dplyr::bind_rows(species_info$names, .id = "key"), key) |>
  knitr::kable()
```

Get the most common result, not filtering on `species` as user supplied name
may have misspelled.

```{r}
(species_data <- 
  dplyr::filter(
    dplyr::count(
      dplyr::filter(species_info$data,
                    !is.na(.data$nubKey)),
        species, nubKey),
    n == max(n)))
```

Use this species key (nubKey) to pull data.

```{r}
species_key <- species_data$nubKey[1]
```

```{r}
knitr::knit_exit()
```

## Getting Occurrence Data 

See package `rgbif`
[Getting Occurrence Data From GBIF](https://docs.ropensci.org/rgbif/articles/getting_occurrence_data.html)
for suggested use.
**This part not proofed yet.**

```{r}
# Download data from GBIF
gbif_pattern <- file.path(gbif_dir, '*.csv')

if (length(list.files(gbif_dir, pattern = "*.csv")) == 0) {
    # Only submit one request
    if (is.null(Sys.getenv("GBIF_DOWNLOAD_KEY")) ||
        0 == length(Sys.getenv("GBIF_DOWNLOAD_KEY"))) {
        # Submit query to GBIF
        gbif_query <- rgbif::occ_download(
#          user = Sys.getenv("GBIF_USER"),
#          pwd = Sys.getenv("GBIF_PWD"),
#          email = Sys.getenv("GBIF_EMAIL"),
            rgbif::pred("speciesKey", species_key),
            rgbif::pred_default(),
            rgbif::pred("year", 2023)
        )
    }

    # Wait for the download to build
    wait <- rgbif::occ_download_meta(gbif_query)
    while (wait$status == 'PREPARING') { # SUCCEEDED
        wait <- rgbif::occ_download_meta(download_key)
        Sys.sleep(5)
    }

    # Download GBIF data
    rgbif::occ_download_get(wait$key, path = data_dir)
}
# Print DOI
cat("DOI", wait$doi)

# Find the extracted .zip file path (take the first result)
gbif_path <- list.files(gbif_dir, pattern = "*.zip", full.names = TRUE)[1]
gbif_path
```

We can use ZIP file directly.

```{r}
gbif_path <- list.files(file.path(gbif_dir, "sandhill"),
                        pattern = "*.zip", full.names = TRUE)[1]
```

```{r}
# Read the GBIF data
gbif_df <- readr::read_delim(
    gbif_path, 
    delim = "\t",
    col_names = TRUE,
    col_types = readr::cols(.default = "c"),
    skip = 0) |>
  dplyr::select(gbifID, month, year, countryCode, stateProvince, decimalLatitude, decimalLongitude)
```

Probably do not need individual observations.

```{r}
gdf_monthly <- dplyr::count(gbif_df, month, year,
  countryCode, stateProvince, decimalLatitude, decimalLongitude)
```

**Not tested past here.**

```{r}
# Set up the ecoregion boundary URL
ecoregions_url <- "https://storage.googleapis.com/teow2016/Ecoregions2017.zip"

# Set up a path to save the data on your machine
ecoregions_dir <- file.path(data_dir, 'wwf_ecoregions')

# Make the ecoregions directory
dir.create(ecoregions_dir, showWarnings = FALSE, recursive = TRUE)

# Join ecoregions shapefile path
ecoregions_path <- file.path(ecoregions_dir, 'wwf_ecoregions.shp')

# Only download once. Do this by hand: go to URL, download file, unzip.
if (!file.exists(ecoregions_path)) {
    ecoregions_gdf <- sf::st_read(ecoregions_url)
    sf::st_write(ecoregions_gdf, ecoregions_path)
}
```
